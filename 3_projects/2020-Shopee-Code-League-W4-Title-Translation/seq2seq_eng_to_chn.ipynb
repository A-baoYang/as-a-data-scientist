{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參考資料：\n",
    "- https://blog.csdn.net/PIPIXIU/article/details/81016974\n",
    "    - 建模講解很完整\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "    - 補齊上一篇缺少模型訓練及參數命名錯誤的部分\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from string import digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import tensorflow\n",
    "# Error Solved: Fail to find the dnn implementation\n",
    "# => https://github.com/tensorflow/tensorflow/issues/24496\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import pickle as pkl\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_eng_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "#     text = re.sub(str([x for x in digits]), \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_che_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[，。、？！：；「」《》·‘•“”?]\", \"\", text)\n",
    "    text = re.sub(r\"\\u200b\", \"\", text)\n",
    "    text = re.sub(str([x for x in digits]), \"\", text)\n",
    "    text = re.sub(str([x for x in string.ascii_lowercase]), \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess(text_list):\n",
    "    text_ = [x.lower() for x in text_list]\n",
    "    text_ = [re.sub(\"'\", '', x) for x in text_]\n",
    "    return text_\n",
    "\n",
    "def removePunc(text_list):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    remove_punc_text = []\n",
    "    for sent in text_list:\n",
    "        sentence = [w.translate(table) for w in sent.split(' ')]\n",
    "        remove_punc_text.append(' '.join(sentence))\n",
    "    return remove_punc_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'language_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df.columns = ['inputs', 'targets']\n",
    "\n",
    "# df.shape\n",
    "\n",
    "input_sentences = df.inputs.values.tolist()#[:NUM_SAMPLE]\n",
    "target_sentences = df.targets.values.tolist()#[:NUM_SAMPLE]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [clean_eng_text(x) for x in input_sentences]\n",
    "input_sentences = preprocess(input_sentences)\n",
    "input_sentences = removePunc(input_sentences)\n",
    "target_sentences = [clean_che_text(x) for x in target_sentences]\n",
    "target_sentences = preprocess(target_sentences)\n",
    "target_sentences = removePunc(target_sentences)\n",
    "\n",
    "# 句首加'\\t'當作起始標誌，句末加'\\n'當作終止標誌\n",
    "target_sentences = ['\\t' + x + '\\n' for x in target_sentences]\n",
    "\n",
    "# 確認中英文各自所有的 unique字符\n",
    "input_ = []\n",
    "for x in input_sentences:\n",
    "    for a in x.split(' '):\n",
    "        input_.append(a)\n",
    "input_characters = sorted(pd.DataFrame(input_)[0].unique())\n",
    "target_characters = sorted(list(set(pd.DataFrame(target_sentences)[0].unique().sum())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成 LSTM 三維 input\n",
    "把句子中各字符轉換成 one-hot 編碼，生成LSTM需要的三维输入 `n_samples`, `timestamp`, `one-hot` features\n",
    "\n",
    "- `NUM_SAMPLES`，样本条数，这里是输入的句子条数\n",
    "- `INPUT_LENGTH`，输入数据的时刻t的长度，这里为最长的英文句子长度\n",
    "- `OUTPUT_LENGTH`，输出数据的时刻t的长度，这里为最长的中文句子长度\n",
    "- `INPUT_FEATURE_LENGTH`，每个时刻进入encoder的lstm单元的数据xtxt的维度，这里为英文中出现的字符数\n",
    "- `OUTPUT_FEATURE_LENGTH`，每个时刻进入decoder的lstm单元的数据xtxt的维度，这里为中文中出现的字符数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_SAMPLES: 23444, INUPT_LENGTH: 34, OUTPUT_LENGTH: 44, INPUT_FEATURE_LENGTH: 6626, OUTPUT_FEATURE_LENGTH: 3457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES = int(len(input_sentences))\n",
    "INUPT_LENGTH = int(max([len(x.split(' ')) for x in input_sentences]))\n",
    "OUTPUT_LENGTH = int(max([len(x) for x in target_sentences]))\n",
    "INPUT_FEATURE_LENGTH = int(len(input_characters))\n",
    "OUTPUT_FEATURE_LENGTH = int(len(target_characters))\n",
    "print(f'NUM_SAMPLES: {NUM_SAMPLES}, INUPT_LENGTH: {INUPT_LENGTH}, OUTPUT_LENGTH: {OUTPUT_LENGTH}, INPUT_FEATURE_LENGTH: {INPUT_FEATURE_LENGTH}, OUTPUT_FEATURE_LENGTH: {OUTPUT_FEATURE_LENGTH}')\n",
    "\n",
    "input_dict = {char:index for index, char in enumerate(input_characters)}\n",
    "input_dict_reverse = {index:char for index, char in enumerate(input_characters)}\n",
    "target_dict = {char:index for index, char in enumerate(target_characters)}\n",
    "target_dict_reverse = {index:char for index, char in enumerate(target_characters)}\n",
    "\n",
    "# encoder输入、decoder输入输出初始化为三维向量\n",
    "encoder_input = np.zeros((NUM_SAMPLES, INUPT_LENGTH, INPUT_FEATURE_LENGTH), dtype='uint8')\n",
    "decoder_input = np.zeros((NUM_SAMPLES, OUTPUT_LENGTH, OUTPUT_FEATURE_LENGTH), dtype='uint8')\n",
    "decoder_output = np.zeros((NUM_SAMPLES, OUTPUT_LENGTH, OUTPUT_FEATURE_LENGTH), dtype='uint8')\n",
    "\n",
    "# 將 input_sentence 進行字符級 one-hot 編碼\n",
    "for seq_index, seq in enumerate(input_sentences):\n",
    "    for char_index, char in enumerate(seq.split(' ')):\n",
    "        encoder_input[seq_index, char_index, input_dict[char]] = 1.0\n",
    "\n",
    "# 將 target_sentence 進行字符級 one-hot 編碼\n",
    "for seq_index, seq in enumerate(target_sentences):\n",
    "    for char_index, char in enumerate(seq):\n",
    "        decoder_input[seq_index, char_index, target_dict[char]] = 1.0\n",
    "        if char_index > 0:\n",
    "            # 训练模型时decoder的输入要比输出晚一个时间步，这样才能对输出监督\n",
    "            decoder_output[seq_index, char_index-1, target_dict[char]] = 1.0\n",
    "\n",
    "\n",
    "encoder_input[0]\n",
    "decoder_input[0]\n",
    "decoder_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 構建 encoder-decoder 及 infer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_input, n_output, n_units):\n",
    "    ###encoder\n",
    "    encode_input = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    _, encoder_h, encoder_c = encoder(encode_input)\n",
    "    encoder_state = [encoder_h, encoder_c]\n",
    "\n",
    "    ###decoder\n",
    "    decode_input = Input(shape=(None, n_output))\n",
    "    decoder = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decode_output, _, _ = decoder(decode_input, initial_state=encoder_state)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decode_output = decoder_dense(decode_output)\n",
    "\n",
    "    model = Model([encode_input, decode_input], decode_output)\n",
    "    \n",
    "    encoder_infer = Model(encode_input, encoder_state)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(n_units, ))\n",
    "    decoder_state_input_c = Input(shape=(n_units, ))\n",
    "    decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_infer_output, decoder_infer_state_h, decoder_infer_state_c = decoder(decode_input, initial_state=decoder_state_input)\n",
    "    decoder_infer_state = [decoder_infer_state_h, decoder_infer_state_c]\n",
    "    decoder_infer_output = decoder_dense(decoder_infer_output)\n",
    "    decoder_infer = Model([decode_input] + decoder_state_input, [decoder_infer_output] + decoder_infer_state)\n",
    "\n",
    "    return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建構預測 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chinese(source, encoder_infer, decoder_infer, n_steps, features):\n",
    "    # 先推理 encoder，获得预测输入序列的隐状态\n",
    "    state = encoder_infer.predict(source)\n",
    "    predict_seq = np.zeros((1, 1, features))\n",
    "    # 標記起始符處\n",
    "    predict_seq[0, 0, target_dict['\\t']] = 1\n",
    "    \n",
    "    output = ''\n",
    "    for i in range(n_steps):\n",
    "        yhat, h, c = decoder_infer.predict([predict_seq] + state)\n",
    "        char_index = np.argmax(yhat[0, -1, :])\n",
    "        char = target_dict_reverse[char_index]\n",
    "        output += char  #輸出中文不需空格\n",
    "        state = [h, c]\n",
    "        predict_seq = np.zeros((1, 1, features))\n",
    "        predict_seq[0, 0, char_index] = 1\n",
    "        if char == '\\n':  # 遇到終止符號則停止拚句\n",
    "            break\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18755 samples, validate on 4689 samples\n",
      "Epoch 1/100\n",
      "18755/18755 [==============================] - 31s 2ms/sample - loss: 1.0989 - accuracy: 0.0320 - val_loss: 1.9072 - val_accuracy: 0.0295\n",
      "Epoch 2/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 1.0158 - accuracy: 0.0395 - val_loss: 1.7452 - val_accuracy: 0.0440\n",
      "Epoch 3/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.9184 - accuracy: 0.0528 - val_loss: 1.6843 - val_accuracy: 0.0535\n",
      "Epoch 4/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.8472 - accuracy: 0.0613 - val_loss: 1.6621 - val_accuracy: 0.0590\n",
      "Epoch 5/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.7948 - accuracy: 0.0672 - val_loss: 1.5625 - val_accuracy: 0.0663\n",
      "Epoch 6/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.7521 - accuracy: 0.0721 - val_loss: 1.5203 - val_accuracy: 0.0723\n",
      "Epoch 7/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.7157 - accuracy: 0.0761 - val_loss: 1.4937 - val_accuracy: 0.0755\n",
      "Epoch 8/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.6844 - accuracy: 0.0801 - val_loss: 1.4755 - val_accuracy: 0.0763\n",
      "Epoch 9/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.6566 - accuracy: 0.0836 - val_loss: 1.4901 - val_accuracy: 0.0784\n",
      "Epoch 10/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.6316 - accuracy: 0.0869 - val_loss: 1.4726 - val_accuracy: 0.0808\n",
      "Epoch 11/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.6082 - accuracy: 0.0899 - val_loss: 1.4725 - val_accuracy: 0.0825\n",
      "Epoch 12/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.5864 - accuracy: 0.0927 - val_loss: 1.4311 - val_accuracy: 0.0844\n",
      "Epoch 13/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.5662 - accuracy: 0.0957 - val_loss: 1.4434 - val_accuracy: 0.0860\n",
      "Epoch 14/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.5465 - accuracy: 0.0983 - val_loss: 1.4370 - val_accuracy: 0.0874\n",
      "Epoch 15/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.5280 - accuracy: 0.1010 - val_loss: 1.4344 - val_accuracy: 0.0884\n",
      "Epoch 16/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.5107 - accuracy: 0.1035 - val_loss: 1.4496 - val_accuracy: 0.0874\n",
      "Epoch 17/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.4937 - accuracy: 0.1062 - val_loss: 1.4384 - val_accuracy: 0.0888\n",
      "Epoch 18/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.4768 - accuracy: 0.1086 - val_loss: 1.4828 - val_accuracy: 0.0874\n",
      "Epoch 19/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.4612 - accuracy: 0.1113 - val_loss: 1.4570 - val_accuracy: 0.0882\n",
      "Epoch 20/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.4456 - accuracy: 0.1136 - val_loss: 1.4844 - val_accuracy: 0.0875\n",
      "Epoch 21/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.4306 - accuracy: 0.1162 - val_loss: 1.4655 - val_accuracy: 0.0906\n",
      "Epoch 22/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.4159 - accuracy: 0.1185 - val_loss: 1.4720 - val_accuracy: 0.0902\n",
      "Epoch 23/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.4019 - accuracy: 0.1210 - val_loss: 1.4847 - val_accuracy: 0.0900\n",
      "Epoch 24/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3882 - accuracy: 0.1236 - val_loss: 1.5006 - val_accuracy: 0.0893\n",
      "Epoch 25/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3754 - accuracy: 0.1258 - val_loss: 1.5087 - val_accuracy: 0.0897\n",
      "Epoch 26/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3629 - accuracy: 0.1280 - val_loss: 1.5213 - val_accuracy: 0.0886\n",
      "Epoch 27/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3505 - accuracy: 0.1302 - val_loss: 1.5343 - val_accuracy: 0.0897\n",
      "Epoch 28/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3383 - accuracy: 0.1325 - val_loss: 1.5487 - val_accuracy: 0.0892\n",
      "Epoch 29/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3269 - accuracy: 0.1347 - val_loss: 1.5492 - val_accuracy: 0.0898\n",
      "Epoch 30/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3156 - accuracy: 0.1370 - val_loss: 1.5820 - val_accuracy: 0.0875\n",
      "Epoch 31/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.3049 - accuracy: 0.1389 - val_loss: 1.5688 - val_accuracy: 0.0886\n",
      "Epoch 32/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2943 - accuracy: 0.1412 - val_loss: 1.5758 - val_accuracy: 0.0883\n",
      "Epoch 33/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2845 - accuracy: 0.1432 - val_loss: 1.5741 - val_accuracy: 0.0891\n",
      "Epoch 34/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2750 - accuracy: 0.1448 - val_loss: 1.5895 - val_accuracy: 0.0889\n",
      "Epoch 35/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2653 - accuracy: 0.1469 - val_loss: 1.6297 - val_accuracy: 0.0858\n",
      "Epoch 36/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2562 - accuracy: 0.1487 - val_loss: 1.6092 - val_accuracy: 0.0892\n",
      "Epoch 37/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2475 - accuracy: 0.1504 - val_loss: 1.6343 - val_accuracy: 0.0876\n",
      "Epoch 38/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2393 - accuracy: 0.1522 - val_loss: 1.6343 - val_accuracy: 0.0882\n",
      "Epoch 39/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2310 - accuracy: 0.1539 - val_loss: 1.6383 - val_accuracy: 0.0881\n",
      "Epoch 40/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2230 - accuracy: 0.1556 - val_loss: 1.6571 - val_accuracy: 0.0875\n",
      "Epoch 41/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2153 - accuracy: 0.1573 - val_loss: 1.6689 - val_accuracy: 0.0869\n",
      "Epoch 42/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2080 - accuracy: 0.1589 - val_loss: 1.6852 - val_accuracy: 0.0865\n",
      "Epoch 43/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.2009 - accuracy: 0.1603 - val_loss: 1.6940 - val_accuracy: 0.0850\n",
      "Epoch 44/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1939 - accuracy: 0.1618 - val_loss: 1.7054 - val_accuracy: 0.0871\n",
      "Epoch 45/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1870 - accuracy: 0.1634 - val_loss: 1.7113 - val_accuracy: 0.0869\n",
      "Epoch 46/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1807 - accuracy: 0.1647 - val_loss: 1.7145 - val_accuracy: 0.0863\n",
      "Epoch 47/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1745 - accuracy: 0.1660 - val_loss: 1.7288 - val_accuracy: 0.0869\n",
      "Epoch 48/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1690 - accuracy: 0.1671 - val_loss: 1.7444 - val_accuracy: 0.0855\n",
      "Epoch 49/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1633 - accuracy: 0.1684 - val_loss: 1.7398 - val_accuracy: 0.0867\n",
      "Epoch 50/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1581 - accuracy: 0.1695 - val_loss: 1.7593 - val_accuracy: 0.0859\n",
      "Epoch 51/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1529 - accuracy: 0.1707 - val_loss: 1.7646 - val_accuracy: 0.0854\n",
      "Epoch 52/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1473 - accuracy: 0.1718 - val_loss: 1.7883 - val_accuracy: 0.0852\n",
      "Epoch 53/100\n",
      "18755/18755 [==============================] - 30s 2ms/sample - loss: 0.1429 - accuracy: 0.1728 - val_loss: 1.7777 - val_accuracy: 0.0850\n",
      "Epoch 54/100\n",
      "18755/18755 [==============================] - 30s 2ms/sample - loss: 0.1379 - accuracy: 0.1740 - val_loss: 1.7897 - val_accuracy: 0.0855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1338 - accuracy: 0.1749 - val_loss: 1.7971 - val_accuracy: 0.0861\n",
      "Epoch 56/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1289 - accuracy: 0.1759 - val_loss: 1.8089 - val_accuracy: 0.0854\n",
      "Epoch 57/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1249 - accuracy: 0.1768 - val_loss: 1.8092 - val_accuracy: 0.0856\n",
      "Epoch 58/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1208 - accuracy: 0.1777 - val_loss: 1.8187 - val_accuracy: 0.0846\n",
      "Epoch 59/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1170 - accuracy: 0.1786 - val_loss: 1.8261 - val_accuracy: 0.0850\n",
      "Epoch 60/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1132 - accuracy: 0.1794 - val_loss: 1.8302 - val_accuracy: 0.0847\n",
      "Epoch 61/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1096 - accuracy: 0.1802 - val_loss: 1.8367 - val_accuracy: 0.0845\n",
      "Epoch 62/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1066 - accuracy: 0.1807 - val_loss: 1.8366 - val_accuracy: 0.0843\n",
      "Epoch 63/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.1032 - accuracy: 0.1814 - val_loss: 1.8562 - val_accuracy: 0.0846\n",
      "Epoch 64/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0997 - accuracy: 0.1822 - val_loss: 1.8486 - val_accuracy: 0.0852\n",
      "Epoch 65/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0971 - accuracy: 0.1827 - val_loss: 1.8598 - val_accuracy: 0.0849\n",
      "Epoch 66/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0937 - accuracy: 0.1836 - val_loss: 1.8645 - val_accuracy: 0.0850\n",
      "Epoch 67/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0911 - accuracy: 0.1841 - val_loss: 1.8734 - val_accuracy: 0.0841\n",
      "Epoch 68/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0884 - accuracy: 0.1846 - val_loss: 1.8769 - val_accuracy: 0.0839\n",
      "Epoch 69/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0855 - accuracy: 0.1854 - val_loss: 1.8857 - val_accuracy: 0.0838\n",
      "Epoch 70/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0833 - accuracy: 0.1857 - val_loss: 1.8956 - val_accuracy: 0.0838\n",
      "Epoch 71/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0807 - accuracy: 0.1862 - val_loss: 1.8918 - val_accuracy: 0.0846\n",
      "Epoch 72/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0784 - accuracy: 0.1866 - val_loss: 1.8921 - val_accuracy: 0.0843\n",
      "Epoch 73/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0765 - accuracy: 0.1870 - val_loss: 1.9130 - val_accuracy: 0.0837\n",
      "Epoch 74/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0741 - accuracy: 0.1875 - val_loss: 1.9201 - val_accuracy: 0.0837\n",
      "Epoch 75/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0719 - accuracy: 0.1881 - val_loss: 1.9219 - val_accuracy: 0.0843\n",
      "Epoch 76/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0700 - accuracy: 0.1883 - val_loss: 1.9145 - val_accuracy: 0.0842\n",
      "Epoch 77/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0684 - accuracy: 0.1886 - val_loss: 1.9165 - val_accuracy: 0.0841\n",
      "Epoch 78/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0663 - accuracy: 0.1890 - val_loss: 1.9277 - val_accuracy: 0.0845\n",
      "Epoch 79/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0644 - accuracy: 0.1895 - val_loss: 1.9289 - val_accuracy: 0.0835\n",
      "Epoch 80/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0627 - accuracy: 0.1897 - val_loss: 1.9449 - val_accuracy: 0.0844\n",
      "Epoch 81/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0611 - accuracy: 0.1900 - val_loss: 1.9553 - val_accuracy: 0.0840\n",
      "Epoch 82/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0594 - accuracy: 0.1904 - val_loss: 1.9373 - val_accuracy: 0.0840\n",
      "Epoch 83/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0579 - accuracy: 0.1907 - val_loss: 1.9564 - val_accuracy: 0.0833\n",
      "Epoch 84/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0563 - accuracy: 0.1910 - val_loss: 1.9580 - val_accuracy: 0.0840\n",
      "Epoch 85/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0550 - accuracy: 0.1911 - val_loss: 1.9535 - val_accuracy: 0.0837\n",
      "Epoch 86/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0539 - accuracy: 0.1914 - val_loss: 1.9665 - val_accuracy: 0.0832\n",
      "Epoch 87/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0521 - accuracy: 0.1917 - val_loss: 1.9663 - val_accuracy: 0.0839\n",
      "Epoch 88/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0511 - accuracy: 0.1919 - val_loss: 1.9737 - val_accuracy: 0.0842\n",
      "Epoch 89/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0499 - accuracy: 0.1921 - val_loss: 1.9703 - val_accuracy: 0.0847\n",
      "Epoch 90/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0487 - accuracy: 0.1923 - val_loss: 1.9771 - val_accuracy: 0.0845\n",
      "Epoch 91/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0476 - accuracy: 0.1926 - val_loss: 1.9827 - val_accuracy: 0.0833\n",
      "Epoch 92/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0465 - accuracy: 0.1927 - val_loss: 1.9848 - val_accuracy: 0.0838\n",
      "Epoch 93/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0452 - accuracy: 0.1929 - val_loss: 1.9743 - val_accuracy: 0.0839\n",
      "Epoch 94/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0443 - accuracy: 0.1931 - val_loss: 1.9884 - val_accuracy: 0.0838\n",
      "Epoch 95/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0431 - accuracy: 0.1933 - val_loss: 1.9936 - val_accuracy: 0.0831\n",
      "Epoch 96/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0422 - accuracy: 0.1935 - val_loss: 1.9972 - val_accuracy: 0.0836\n",
      "Epoch 97/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0414 - accuracy: 0.1936 - val_loss: 1.9994 - val_accuracy: 0.0835\n",
      "Epoch 98/100\n",
      "18755/18755 [==============================] - 29s 2ms/sample - loss: 0.0404 - accuracy: 0.1938 - val_loss: 1.9825 - val_accuracy: 0.0840\n",
      "Epoch 99/100\n",
      "18755/18755 [==============================] - 30s 2ms/sample - loss: 0.0394 - accuracy: 0.1939 - val_loss: 2.0150 - val_accuracy: 0.0826\n",
      "Epoch 100/100\n",
      "18755/18755 [==============================] - 30s 2ms/sample - loss: 0.0389 - accuracy: 0.1940 - val_loss: 1.9960 - val_accuracy: 0.0839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d9d6af648>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, encoder_infer, decoder_infer = create_model(n_input=INPUT_FEATURE_LENGTH,\n",
    "                                                   n_output=OUTPUT_FEATURE_LENGTH,\n",
    "                                                   n_units=latent_dim)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input, decoder_input], decoder_output, \n",
    "          batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "model.save('seq2seq.h5')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 進行英翻中推論預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 34, 6626)\n",
      "i like reading\n",
      "我喜欢阅读\n",
      "\n",
      "(1, 34, 6626)\n",
      "i like running\n",
      "我喜欢跑步\n",
      "\n",
      "(1, 34, 6626)\n",
      "i like to read\n",
      "我喜欢阅读\n",
      "\n",
      "(1, 34, 6626)\n",
      "i love my life\n",
      "我爱我的生活\n",
      "\n",
      "(1, 34, 6626)\n",
      "i love my life\n",
      "我爱我的生活\n",
      "\n",
      "(1, 34, 6626)\n",
      "i love my wife\n",
      "我爱我我的妻子\n",
      "\n",
      "(1, 34, 6626)\n",
      "i love parties\n",
      "我愛派對\n",
      "\n",
      "(1, 34, 6626)\n",
      "i met a friend\n",
      "我遇见一个朋友\n",
      "\n",
      "(1, 34, 6626)\n",
      "i must decline\n",
      "我必须拒绝\n",
      "\n",
      "(1, 34, 6626)\n",
      "i must go home\n",
      "我该回家了\n",
      "\n",
      "(1, 34, 6626)\n",
      "i need a raise\n",
      "我需要加帮大\n",
      "\n",
      "(1, 34, 6626)\n",
      "i need a stamp\n",
      "我需要一張郵票\n",
      "\n",
      "(1, 34, 6626)\n",
      "i need it asap\n",
      "我尽快需要\n",
      "\n",
      "(1, 34, 6626)\n",
      "i need my coat\n",
      "我需要我的大衣\n",
      "\n",
      "(1, 34, 6626)\n",
      "i need to know\n",
      "我需要知道\n",
      "\n",
      "(1, 34, 6626)\n",
      "i often hiccup\n",
      "我经常打嗝\n",
      "\n",
      "(1, 34, 6626)\n",
      "i said shut up\n",
      "我说过了闭嘴\n",
      "\n",
      "(1, 34, 6626)\n",
      "i saw five men\n",
      "我看见五个男人\n",
      "\n",
      "(1, 34, 6626)\n",
      "i saw five men\n",
      "我看见五个男人\n",
      "\n",
      "(1, 34, 6626)\n",
      "i saw her swim\n",
      "我看見她游泳\n",
      "\n",
      "(1, 34, 6626)\n",
      "i should do it\n",
      "我该做什么\n",
      "\n",
      "(1, 34, 6626)\n",
      "i study korean\n",
      "我学来了\n",
      "\n",
      "(1, 34, 6626)\n",
      "i want to play\n",
      "我想去玩\n",
      "\n",
      "(1, 34, 6626)\n",
      "i was a doctor\n",
      "我以前是醫生\n",
      "\n",
      "(1, 34, 6626)\n",
      "i was learning\n",
      "我在學習\n",
      "\n",
      "(1, 34, 6626)\n",
      "i was not drunk\n",
      "我沒醉\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will sue you\n",
      "我要告你\n",
      "\n",
      "(1, 34, 6626)\n",
      "i would like to go\n",
      "我想要去\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will alert tom\n",
      "我会警告汤姆\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will come back\n",
      "我会回来的\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will scold him\n",
      "我會責備他\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will see to it\n",
      "我會留意的\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will see to it\n",
      "我會留意的\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will stay home\n",
      "我會待在家裡\n",
      "\n",
      "(1, 34, 6626)\n",
      "i will treat you\n",
      "我請你\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am a free man\n",
      "我是一個自由的人\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am a good guy\n",
      "我是一个好人\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am an atheist\n",
      "我是无神论者\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am behind him\n",
      "我在他後面\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am busy today\n",
      "我今天忙\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am dead tired\n",
      "我累死了\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am free today\n",
      "我今天有空\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am no quitter\n",
      "我不輕易放棄\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am not famous\n",
      "我不出名\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am not famous\n",
      "我不出名\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am not greedy\n",
      "我不会说法语\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am not guilty\n",
      "我没有罪\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am not stupid\n",
      "我不是个派对不懂\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am so excited\n",
      "我很激动\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am undressing\n",
      "我總是有黑色的眼睛\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am very happy\n",
      "我很快乐\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am very happy\n",
      "我很快乐\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am very happy\n",
      "我很快乐\n",
      "\n",
      "(1, 34, 6626)\n",
      "i am very happy\n",
      "我很快乐\n",
      "\n",
      "(1, 34, 6626)\n",
      "i have seen that\n",
      "我見過\n",
      "\n",
      "(1, 34, 6626)\n",
      "is he japanese\n",
      "他是日本人吗\n",
      "\n",
      "(1, 34, 6626)\n",
      "is that better\n",
      "那更好吗\n",
      "\n",
      "(1, 34, 6626)\n",
      "is this edible\n",
      "这个能吃吗\n",
      "\n",
      "(1, 34, 6626)\n",
      "is not it black\n",
      "它不是黑色的嗎\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is so early\n",
      "太早了\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is too late\n",
      "太晚了\n",
      "\n",
      "(1, 34, 6626)\n",
      "it makes sense\n",
      "那样说得通\n",
      "\n",
      "(1, 34, 6626)\n",
      "it took months\n",
      "它花了几个月\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is brand new\n",
      "这是全新的\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is dangerous\n",
      "它是危險的\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is delicious\n",
      "很美味\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is not funny\n",
      "这不好笑\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is our fault\n",
      "它是我們的錯誤\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is the truth\n",
      "这是事实\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is too large\n",
      "它太大了\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is up to you\n",
      "由你來決定\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is up to you\n",
      "由你來決定\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is very cold\n",
      "非常冷\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is well done\n",
      "做得很好\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is your book\n",
      "它是你的書\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is your move\n",
      "該你走了\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is your turn\n",
      "輪到你了\n",
      "\n",
      "(1, 34, 6626)\n",
      "it is your turn\n",
      "輪到你了\n",
      "\n",
      "(1, 34, 6626)\n",
      "keep listening\n",
      "继续听\n",
      "\n",
      "(1, 34, 6626)\n",
      "let me come in\n",
      "让我去看\n",
      "\n",
      "(1, 34, 6626)\n",
      "let me do that\n",
      "让我去做\n",
      "\n",
      "(1, 34, 6626)\n",
      "let us go home\n",
      "让我们回家吧\n",
      "\n",
      "(1, 34, 6626)\n",
      "lets just eat\n",
      "我們吃吧\n",
      "\n",
      "(1, 34, 6626)\n",
      "listen to this\n",
      "聽聽這個\n",
      "\n",
      "(1, 34, 6626)\n",
      "louder please\n",
      "请大声\n",
      "\n",
      "(1, 34, 6626)\n",
      "lunch is ready\n",
      "午餐好了\n",
      "\n",
      "(1, 34, 6626)\n",
      "may i help you\n",
      "我能幫你嗎\n",
      "\n",
      "(1, 34, 6626)\n",
      "may i use this\n",
      "我可以用嗎\n",
      "\n",
      "(1, 34, 6626)\n",
      "my name is tom\n",
      "我叫tom\n",
      "\n",
      "(1, 34, 6626)\n",
      "now i remember\n",
      "现在我想起来了\n",
      "\n",
      "(1, 34, 6626)\n",
      "please come in\n",
      "请回来\n",
      "\n",
      "(1, 34, 6626)\n",
      "please do that\n",
      "請這麼快樂\n",
      "\n",
      "(1, 34, 6626)\n",
      "please help me\n",
      "請幫我\n",
      "\n",
      "(1, 34, 6626)\n",
      "please help me\n",
      "請幫我\n",
      "\n",
      "(1, 34, 6626)\n",
      "please join us\n",
      "請加入我們\n",
      "\n",
      "(1, 34, 6626)\n",
      "please tell me\n",
      "請告訴我\n",
      "\n",
      "(1, 34, 6626)\n",
      "please wash it\n",
      "请清洗它\n",
      "\n",
      "(1, 34, 6626)\n",
      "prices went up\n",
      "物價上下雨了\n",
      "\n",
      "(1, 34, 6626)\n",
      "read this book\n",
      "看这本书\n",
      "\n",
      "(1, 34, 6626)\n",
      "say it clearly\n",
      "說清楚\n",
      "\n",
      "(1, 34, 6626)\n",
      "science is fun\n",
      "科學好玩\n",
      "\n",
      "(1, 34, 6626)\n",
      "see you around\n",
      "再见\n",
      "\n",
      "(1, 34, 6626)\n",
      "she fooled him\n",
      "她一定地方法向\n",
      "\n",
      "(1, 34, 6626)\n",
      "she grew roses\n",
      "她種了玫瑰\n",
      "\n",
      "(1, 34, 6626)\n",
      "she hugged him\n",
      "她抱了他\n",
      "\n",
      "(1, 34, 6626)\n",
      "she is dieting\n",
      "她在节食\n",
      "\n",
      "(1, 34, 6626)\n",
      "she is dieting\n",
      "她在节食\n",
      "\n",
      "(1, 34, 6626)\n",
      "she is walking\n",
      "她在行走\n",
      "\n",
      "(1, 34, 6626)\n",
      "she might come\n",
      "她也許會來\n",
      "\n",
      "(1, 34, 6626)\n",
      "she seems rich\n",
      "她看來有錢\n",
      "\n",
      "(1, 34, 6626)\n",
      "shut your eyes\n",
      "闭上你们的眼睛\n",
      "\n",
      "(1, 34, 6626)\n",
      "smoke appeared\n",
      "烟烟\n",
      "\n",
      "(1, 34, 6626)\n",
      "someone called\n",
      "有人打電話來\n",
      "\n",
      "(1, 34, 6626)\n",
      "stop grumbling\n",
      "停止发當了\n",
      "\n",
      "(1, 34, 6626)\n",
      "stop resisting\n",
      "停止抵抗\n",
      "\n",
      "(1, 34, 6626)\n",
      "summer is over\n",
      "夏天过去了\n",
      "\n",
      "(1, 34, 6626)\n",
      "take your time\n",
      "你可以慢慢来\n",
      "\n",
      "(1, 34, 6626)\n",
      "take your time\n",
      "你可以慢慢来\n",
      "\n",
      "(1, 34, 6626)\n",
      "that was wrong\n",
      "那样是错的\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is a shame\n",
      "那是一個很好的次\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is logical\n",
      "那裡很多人\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is my coat\n",
      "那是我的大衣\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is perfect\n",
      "那是完美的\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is too bad\n",
      "那太糟糕了\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is too bad\n",
      "那太糟糕了\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is too bad\n",
      "那太糟糕了\n",
      "\n",
      "(1, 34, 6626)\n",
      "the birds sang\n",
      "鳥兒歌唱\n",
      "\n",
      "(1, 34, 6626)\n",
      "the flag is up\n",
      "人是會兒他的快\n",
      "\n",
      "(1, 34, 6626)\n",
      "the phone rang\n",
      "電話正在響\n",
      "\n",
      "(1, 34, 6626)\n",
      "their eyes met\n",
      "他們的小說孩子的相過很難\n",
      "\n",
      "(1, 34, 6626)\n",
      "these are pens\n",
      "這些是筆\n",
      "\n",
      "(1, 34, 6626)\n",
      "they hated tom\n",
      "他們恨湯姆\n",
      "\n",
      "(1, 34, 6626)\n",
      "they have jobs\n",
      "他們有工作\n",
      "\n",
      "(1, 34, 6626)\n",
      "they let me go\n",
      "他們讓我走\n",
      "\n",
      "(1, 34, 6626)\n",
      "they love that\n",
      "他們喜歡麼們\n",
      "\n",
      "(1, 34, 6626)\n",
      "they trust tom\n",
      "他们信任汤姆\n",
      "\n",
      "(1, 34, 6626)\n",
      "they want more\n",
      "他們想要更多\n",
      "\n",
      "(1, 34, 6626)\n",
      "they want this\n",
      "他們想要這個\n",
      "\n",
      "(1, 34, 6626)\n",
      "they were good\n",
      "他们都们\n",
      "\n",
      "(1, 34, 6626)\n",
      "this is a book\n",
      "这是一本书\n",
      "\n",
      "(1, 34, 6626)\n",
      "this is my bag\n",
      "那是我的包\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom can change\n",
      "湯姆能改變\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom cannot swim\n",
      "汤姆不会游泳\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom has a plan\n",
      "湯姆有個計畫\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom has brains\n",
      "汤姆脑子好使\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom has failed\n",
      "汤姆失败了\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom is a rabbi\n",
      "汤姆是个拉比\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom is no fool\n",
      "汤姆不傻\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom is sloshed\n",
      "汤姆醉了\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom is smashed\n",
      "汤姆醉了\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom is not dumb\n",
      "汤姆不傻\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom lives here\n",
      "汤姆住在这里\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom looks pale\n",
      "湯姆看起來很蒼白\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom loves dogs\n",
      "汤姆喜欢狗\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom turned red\n",
      "汤姆脸红了\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom walked out\n",
      "湯姆走了出去\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom was crying\n",
      "汤姆当时在哭\n",
      "\n",
      "(1, 34, 6626)\n",
      "tom will not stop\n",
      "湯姆不會停\n",
      "\n",
      "(1, 34, 6626)\n",
      "toms fearless\n",
      "汤姆无所有感冒\n",
      "\n",
      "(1, 34, 6626)\n",
      "toms laughing\n",
      "汤姆在笑\n",
      "\n",
      "(1, 34, 6626)\n",
      "toms out cold\n",
      "汤姆正昏迷不醒\n",
      "\n",
      "(1, 34, 6626)\n",
      "toms thrilled\n",
      "汤姆兴奋不已\n",
      "\n",
      "(1, 34, 6626)\n",
      "turn on the tv\n",
      "开电视\n",
      "\n",
      "(1, 34, 6626)\n",
      "turn up the tv\n",
      "把电视声音调大点儿\n",
      "\n",
      "(1, 34, 6626)\n",
      "was tom asleep\n",
      "汤姆睡着了吗\n",
      "\n",
      "(1, 34, 6626)\n",
      "wash your feet\n",
      "洗你的脚\n",
      "\n",
      "(1, 34, 6626)\n",
      "wash your feet\n",
      "洗你的脚\n",
      "\n",
      "(1, 34, 6626)\n",
      "watch yourself\n",
      "自己当心啊\n",
      "\n",
      "(1, 34, 6626)\n",
      "we forgive you\n",
      "我們原諒你\n",
      "\n",
      "(1, 34, 6626)\n",
      "we knew no one\n",
      "我們誰也不認識\n",
      "\n",
      "(1, 34, 6626)\n",
      "we need a hero\n",
      "我們需要英雄\n",
      "\n",
      "(1, 34, 6626)\n",
      "we study music\n",
      "我们学习音乐\n",
      "\n",
      "(1, 34, 6626)\n",
      "we were robbed\n",
      "我們被偷了\n",
      "\n",
      "(1, 34, 6626)\n",
      "we will continue\n",
      "我们要继续下去\n",
      "\n",
      "(1, 34, 6626)\n",
      "we are a family\n",
      "我們是個家庭\n",
      "\n",
      "(1, 34, 6626)\n",
      "we are not late\n",
      "我们没迟到\n",
      "\n",
      "(1, 34, 6626)\n",
      "well lets go\n",
      "好吧我們走吧\n",
      "\n",
      "(1, 34, 6626)\n",
      "were you right\n",
      "你是对的吗\n",
      "\n",
      "(1, 34, 6626)\n",
      "what about you\n",
      "你們呢\n",
      "\n",
      "(1, 34, 6626)\n",
      "what about you\n",
      "你們呢\n",
      "\n",
      "(1, 34, 6626)\n",
      "what do you do\n",
      "你的職業是什麼\n",
      "\n",
      "(1, 34, 6626)\n",
      "that is her job\n",
      "她做什么工作\n",
      "\n",
      "(1, 34, 6626)\n",
      "where do we go\n",
      "我们去哪儿\n",
      "\n",
      "(1, 34, 6626)\n",
      "where were you\n",
      "你在哪儿\n",
      "\n",
      "(1, 34, 6626)\n",
      "whos that guy\n",
      "那家伙是谁\n",
      "\n",
      "(1, 34, 6626)\n",
      "whos that man\n",
      "那男人是谁\n",
      "\n",
      "(1, 34, 6626)\n",
      "why do you ask\n",
      "你為什麼问\n",
      "\n",
      "(1, 34, 6626)\n",
      "why is he here\n",
      "为什么他在这儿\n",
      "\n",
      "(1, 34, 6626)\n",
      "wipe your eyes\n",
      "該你好\n",
      "\n",
      "(1, 34, 6626)\n",
      "yes i know it\n",
      "是的我知道\n",
      "\n",
      "(1, 34, 6626)\n",
      "yes of course\n",
      "是的當然\n",
      "\n",
      "(1, 34, 6626)\n",
      "you look bored\n",
      "你看起來很無聊\n",
      "\n",
      "(1, 34, 6626)\n",
      "you look tense\n",
      "你看起来很紧张\n",
      "\n",
      "(1, 34, 6626)\n",
      "you look tired\n",
      "你看起来很困了\n",
      "\n",
      "(1, 34, 6626)\n",
      "you look tired\n",
      "你看起来很困了\n",
      "\n",
      "(1, 34, 6626)\n",
      "you must do it\n",
      "你必須去做\n",
      "\n",
      "(1, 34, 6626)\n",
      "you will love it\n",
      "你會愛它\n",
      "\n",
      "(1, 34, 6626)\n",
      "you are kidding\n",
      "你開玩笑吧\n",
      "\n",
      "(1, 34, 6626)\n",
      "you are welcome\n",
      "你不必感谢我\n",
      "\n",
      "(1, 34, 6626)\n",
      "a man must work\n",
      "男人应该工作\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000, 1200):\n",
    "    test = encoder_input[i:i+1, :, :]\n",
    "    print(test.shape)\n",
    "    out = predict_chinese(test, encoder_infer, decoder_infer, OUTPUT_LENGTH, OUTPUT_FEATURE_LENGTH)\n",
    "    print(input_sentences[i])\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
