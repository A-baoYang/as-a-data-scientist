{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 根據 `seq2seq_eng_to_chn.ipynb` 改寫成 中翻英"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from string import digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import tensorflow\n",
    "# Error Solved: Fail to find the dnn implementation\n",
    "# => https://github.com/tensorflow/tensorflow/issues/24496\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import pickle as pkl\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_eng_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "#     text = re.sub(str([x for x in digits]), \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_che_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[，。、？！：；「」《》·‘•“”?]\", \"\", text)\n",
    "    text = re.sub(r\"\\u200b\", \"\", text)\n",
    "    text = re.sub(str([x for x in digits]), \"\", text)\n",
    "    text = re.sub(str([x for x in string.ascii_lowercase]), \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess(text_list):\n",
    "    text_ = [x.lower() for x in text_list]\n",
    "    text_ = [re.sub(\"'\", '', x) for x in text_]\n",
    "    return text_\n",
    "\n",
    "def removePunc(text_list):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    remove_punc_text = []\n",
    "    for sent in text_list:\n",
    "        sentence = [w.translate(table) for w in sent.split(' ')]\n",
    "        remove_punc_text.append(' '.join(sentence))\n",
    "    return remove_punc_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'language_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df.columns = ['targets', 'inputs']\n",
    "\n",
    "# df.shape\n",
    "\n",
    "input_sentences = df.inputs.values.tolist()#[:NUM_SAMPLE]\n",
    "target_sentences = df.targets.values.tolist()#[:NUM_SAMPLE]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = [clean_che_text(x) for x in input_sentences]\n",
    "input_sentences = preprocess(input_sentences)\n",
    "input_sentences = removePunc(input_sentences)\n",
    "target_sentences = [clean_eng_text(x) for x in target_sentences]\n",
    "target_sentences = preprocess(target_sentences)\n",
    "target_sentences = removePunc(target_sentences)\n",
    "\n",
    "# 句首加'\\t'當作起始標誌，句末加'\\n'當作終止標誌\n",
    "target_sentences = ['\\t ' + x + ' \\n' for x in target_sentences]\n",
    "\n",
    "# 確認中英文各自所有的 unique字符\n",
    "target_ = []\n",
    "for x in target_sentences:\n",
    "    for a in x.split(' '):\n",
    "        target_.append(a)\n",
    "\n",
    "input_characters = sorted(list(set(pd.DataFrame(input_sentences)[0].unique().sum())))\n",
    "target_characters = sorted(pd.DataFrame(target_)[0].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成 LSTM 三維 input\n",
    "把句子中各字符轉換成 one-hot 編碼，生成LSTM需要的三维输入 `n_samples`, `timestamp`, `one-hot` features\n",
    "\n",
    "- `NUM_SAMPLES`，样本条数，这里是输入的句子条数\n",
    "- `INPUT_LENGTH`，输入数据的时刻t的长度，这里为最长的英文句子长度\n",
    "- `OUTPUT_LENGTH`，输出数据的时刻t的长度，这里为最长的中文句子长度\n",
    "- `INPUT_FEATURE_LENGTH`，每个时刻进入encoder的lstm单元的数据xtxt的维度，这里为英文中出现的字符数\n",
    "- `OUTPUT_FEATURE_LENGTH`，每个时刻进入decoder的lstm单元的数据xtxt的维度，这里为中文中出现的字符数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_SAMPLES: 23444, INUPT_LENGTH: 42, OUTPUT_LENGTH: 36, INPUT_FEATURE_LENGTH: 3420, OUTPUT_FEATURE_LENGTH: 6628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES = int(len(input_sentences))\n",
    "INUPT_LENGTH = int(max([len(x) for x in input_sentences]))\n",
    "OUTPUT_LENGTH = int(max([len(x.split(' ')) for x in target_sentences]))\n",
    "INPUT_FEATURE_LENGTH = int(len(input_characters))\n",
    "OUTPUT_FEATURE_LENGTH = int(len(target_characters))\n",
    "print(f'NUM_SAMPLES: {NUM_SAMPLES}, INUPT_LENGTH: {INUPT_LENGTH}, OUTPUT_LENGTH: {OUTPUT_LENGTH}, INPUT_FEATURE_LENGTH: {INPUT_FEATURE_LENGTH}, OUTPUT_FEATURE_LENGTH: {OUTPUT_FEATURE_LENGTH}')\n",
    "\n",
    "input_dict = {char:index for index, char in enumerate(input_characters)}\n",
    "input_dict_reverse = {index:char for index, char in enumerate(input_characters)}\n",
    "target_dict = {char:index for index, char in enumerate(target_characters)}\n",
    "target_dict_reverse = {index:char for index, char in enumerate(target_characters)}\n",
    "\n",
    "# encoder输入、decoder输入输出初始化为三维向量\n",
    "encoder_input = np.zeros((NUM_SAMPLES, INUPT_LENGTH, INPUT_FEATURE_LENGTH), dtype='uint8')\n",
    "decoder_input = np.zeros((NUM_SAMPLES, OUTPUT_LENGTH, OUTPUT_FEATURE_LENGTH), dtype='uint8')\n",
    "decoder_output = np.zeros((NUM_SAMPLES, OUTPUT_LENGTH, OUTPUT_FEATURE_LENGTH), dtype='uint8')\n",
    "\n",
    "# 將 input_sentence 進行字符級 one-hot 編碼\n",
    "for seq_index, seq in enumerate(input_sentences):\n",
    "    for char_index, char in enumerate(seq):\n",
    "        encoder_input[seq_index, char_index, input_dict[char]] = 1.0\n",
    "\n",
    "# 將 target_sentence 進行字符級 one-hot 編碼\n",
    "for seq_index, seq in enumerate(target_sentences):\n",
    "    for char_index, char in enumerate(seq.split(' ')):\n",
    "        decoder_input[seq_index, char_index, target_dict[char]] = 1.0\n",
    "        if char_index > 0:\n",
    "            # 训练模型时decoder的输入要比输出晚一个时间步，这样才能对输出监督\n",
    "            decoder_output[seq_index, char_index-1, target_dict[char]] = 1.0\n",
    "\n",
    "\n",
    "encoder_input[0]\n",
    "decoder_input[0]\n",
    "decoder_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 構建 encoder-decoder 及 infer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_input, n_output, n_units):\n",
    "    ###encoder\n",
    "    encode_input = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    _, encoder_h, encoder_c = encoder(encode_input)\n",
    "    encoder_state = [encoder_h, encoder_c]\n",
    "\n",
    "    ###decoder\n",
    "    decode_input = Input(shape=(None, n_output))\n",
    "    decoder = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decode_output, _, _ = decoder(decode_input, initial_state=encoder_state)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decode_output = decoder_dense(decode_output)\n",
    "\n",
    "    model = Model([encode_input, decode_input], decode_output)\n",
    "    \n",
    "    encoder_infer = Model(encode_input, encoder_state)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(n_units, ))\n",
    "    decoder_state_input_c = Input(shape=(n_units, ))\n",
    "    decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_infer_output, decoder_infer_state_h, decoder_infer_state_c = decoder(decode_input, initial_state=decoder_state_input)\n",
    "    decoder_infer_state = [decoder_infer_state_h, decoder_infer_state_c]\n",
    "    decoder_infer_output = decoder_dense(decoder_infer_output)\n",
    "    decoder_infer = Model([decode_input] + decoder_state_input, [decoder_infer_output] + decoder_infer_state)\n",
    "\n",
    "    return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建構預測 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chinese(source, encoder_infer, decoder_infer, n_steps, features):\n",
    "    # 先推理 encoder，获得预测输入序列的隐状态\n",
    "    state = encoder_infer.predict(source)\n",
    "    predict_seq = np.zeros((1, 1, features))\n",
    "    # 標記起始符處\n",
    "    predict_seq[0, 0, target_dict['\\t']] = 1\n",
    "    \n",
    "    output = ''\n",
    "    for i in range(n_steps):\n",
    "        yhat, h, c = decoder_infer.predict([predict_seq] + state)\n",
    "        char_index = np.argmax(yhat[0, -1, :])\n",
    "        char = target_dict_reverse[char_index]\n",
    "        output += char + ' '  #輸出中文不需空格\n",
    "        state = [h, c]\n",
    "        predict_seq = np.zeros((1, 1, features))\n",
    "        predict_seq[0, 0, char_index] = 1\n",
    "        if char == '\\n':  # 遇到終止符號則停止拚句\n",
    "            break\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18755 samples, validate on 4689 samples\n",
      "Epoch 1/100\n",
      "18755/18755 [==============================] - 36s 2ms/sample - loss: 0.9778 - accuracy: 0.0378 - val_loss: 1.7075 - val_accuracy: 0.0389\n",
      "Epoch 2/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.8984 - accuracy: 0.0428 - val_loss: 1.7014 - val_accuracy: 0.0427\n",
      "Epoch 3/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.8320 - accuracy: 0.0507 - val_loss: 1.6123 - val_accuracy: 0.0515\n",
      "Epoch 4/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.7758 - accuracy: 0.0567 - val_loss: 1.6304 - val_accuracy: 0.0557\n",
      "Epoch 5/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.7357 - accuracy: 0.0616 - val_loss: 1.5462 - val_accuracy: 0.0615\n",
      "Epoch 6/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.7067 - accuracy: 0.0653 - val_loss: 1.5505 - val_accuracy: 0.0640\n",
      "Epoch 7/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.6796 - accuracy: 0.0691 - val_loss: 1.5434 - val_accuracy: 0.0653\n",
      "Epoch 8/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.6551 - accuracy: 0.0721 - val_loss: 1.5333 - val_accuracy: 0.0662\n",
      "Epoch 9/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.6332 - accuracy: 0.0751 - val_loss: 1.5135 - val_accuracy: 0.0697\n",
      "Epoch 10/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.6113 - accuracy: 0.0780 - val_loss: 1.5320 - val_accuracy: 0.0706\n",
      "Epoch 11/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.5912 - accuracy: 0.0807 - val_loss: 1.5331 - val_accuracy: 0.0696\n",
      "Epoch 12/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.5719 - accuracy: 0.0834 - val_loss: 1.5118 - val_accuracy: 0.0732\n",
      "Epoch 13/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.5536 - accuracy: 0.0862 - val_loss: 1.5510 - val_accuracy: 0.0723\n",
      "Epoch 14/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.5358 - accuracy: 0.0888 - val_loss: 1.4708 - val_accuracy: 0.0761\n",
      "Epoch 15/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.5190 - accuracy: 0.0914 - val_loss: 1.4832 - val_accuracy: 0.0769\n",
      "Epoch 16/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.5025 - accuracy: 0.0940 - val_loss: 1.5394 - val_accuracy: 0.0756\n",
      "Epoch 17/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.4867 - accuracy: 0.0966 - val_loss: 1.5228 - val_accuracy: 0.0770\n",
      "Epoch 18/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.4712 - accuracy: 0.0991 - val_loss: 1.5013 - val_accuracy: 0.0799\n",
      "Epoch 19/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.4563 - accuracy: 0.1017 - val_loss: 1.5464 - val_accuracy: 0.0783\n",
      "Epoch 20/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.4414 - accuracy: 0.1041 - val_loss: 1.5137 - val_accuracy: 0.0791\n",
      "Epoch 21/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.4271 - accuracy: 0.1068 - val_loss: 1.4921 - val_accuracy: 0.0816\n",
      "Epoch 22/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.4134 - accuracy: 0.1092 - val_loss: 1.5359 - val_accuracy: 0.0804\n",
      "Epoch 23/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.3994 - accuracy: 0.1119 - val_loss: 1.5685 - val_accuracy: 0.0800\n",
      "Epoch 24/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.3867 - accuracy: 0.1143 - val_loss: 1.5549 - val_accuracy: 0.0818\n",
      "Epoch 25/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.3739 - accuracy: 0.1163 - val_loss: 1.5702 - val_accuracy: 0.0798\n",
      "Epoch 26/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.3621 - accuracy: 0.1186 - val_loss: 1.6022 - val_accuracy: 0.0795\n",
      "Epoch 27/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.3505 - accuracy: 0.1210 - val_loss: 1.5930 - val_accuracy: 0.0807\n",
      "Epoch 28/100\n",
      "18755/18755 [==============================] - 42s 2ms/sample - loss: 0.3390 - accuracy: 0.1230 - val_loss: 1.5835 - val_accuracy: 0.0823\n",
      "Epoch 29/100\n",
      "18755/18755 [==============================] - 36s 2ms/sample - loss: 0.3287 - accuracy: 0.1251 - val_loss: 1.6090 - val_accuracy: 0.0821\n",
      "Epoch 30/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.3182 - accuracy: 0.1274 - val_loss: 1.6512 - val_accuracy: 0.0802\n",
      "Epoch 31/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.3081 - accuracy: 0.1296 - val_loss: 1.6031 - val_accuracy: 0.0821\n",
      "Epoch 32/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2981 - accuracy: 0.1315 - val_loss: 1.6325 - val_accuracy: 0.0826\n",
      "Epoch 33/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2894 - accuracy: 0.1332 - val_loss: 1.6726 - val_accuracy: 0.0798\n",
      "Epoch 34/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2803 - accuracy: 0.1350 - val_loss: 1.6483 - val_accuracy: 0.0806\n",
      "Epoch 35/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2716 - accuracy: 0.1368 - val_loss: 1.6589 - val_accuracy: 0.0820\n",
      "Epoch 36/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2632 - accuracy: 0.1388 - val_loss: 1.6799 - val_accuracy: 0.0806\n",
      "Epoch 37/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2557 - accuracy: 0.1404 - val_loss: 1.6821 - val_accuracy: 0.0807\n",
      "Epoch 38/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2481 - accuracy: 0.1419 - val_loss: 1.7064 - val_accuracy: 0.0809\n",
      "Epoch 39/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2411 - accuracy: 0.1435 - val_loss: 1.6942 - val_accuracy: 0.0813\n",
      "Epoch 40/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2346 - accuracy: 0.1448 - val_loss: 1.7412 - val_accuracy: 0.0810\n",
      "Epoch 41/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2280 - accuracy: 0.1462 - val_loss: 1.7454 - val_accuracy: 0.0814\n",
      "Epoch 42/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2219 - accuracy: 0.1476 - val_loss: 1.7395 - val_accuracy: 0.0810\n",
      "Epoch 43/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2163 - accuracy: 0.1488 - val_loss: 1.7381 - val_accuracy: 0.0817\n",
      "Epoch 44/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2109 - accuracy: 0.1501 - val_loss: 1.7449 - val_accuracy: 0.0814\n",
      "Epoch 45/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.2049 - accuracy: 0.1514 - val_loss: 1.7734 - val_accuracy: 0.0809\n",
      "Epoch 46/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1997 - accuracy: 0.1522 - val_loss: 1.7690 - val_accuracy: 0.0814\n",
      "Epoch 47/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1941 - accuracy: 0.1534 - val_loss: 1.7884 - val_accuracy: 0.0793\n",
      "Epoch 48/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1892 - accuracy: 0.1542 - val_loss: 1.7567 - val_accuracy: 0.0811\n",
      "Epoch 49/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1840 - accuracy: 0.1551 - val_loss: 1.7884 - val_accuracy: 0.0805\n",
      "Epoch 50/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1795 - accuracy: 0.1559 - val_loss: 1.8270 - val_accuracy: 0.0802\n",
      "Epoch 51/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1750 - accuracy: 0.1571 - val_loss: 1.7967 - val_accuracy: 0.0810\n",
      "Epoch 52/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1711 - accuracy: 0.1578 - val_loss: 1.8035 - val_accuracy: 0.0809\n",
      "Epoch 53/100\n",
      "18755/18755 [==============================] - 35s 2ms/sample - loss: 0.1672 - accuracy: 0.1585 - val_loss: 1.8084 - val_accuracy: 0.0793\n",
      "Epoch 54/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1637 - accuracy: 0.1592 - val_loss: 1.8141 - val_accuracy: 0.0806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1599 - accuracy: 0.1600 - val_loss: 1.7988 - val_accuracy: 0.0806\n",
      "Epoch 56/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1566 - accuracy: 0.1606 - val_loss: 1.8343 - val_accuracy: 0.0804\n",
      "Epoch 57/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1537 - accuracy: 0.1610 - val_loss: 1.8400 - val_accuracy: 0.0799\n",
      "Epoch 58/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1501 - accuracy: 0.1618 - val_loss: 1.8541 - val_accuracy: 0.0803\n",
      "Epoch 59/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1472 - accuracy: 0.1622 - val_loss: 1.8533 - val_accuracy: 0.0795\n",
      "Epoch 60/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1442 - accuracy: 0.1628 - val_loss: 1.8491 - val_accuracy: 0.0797\n",
      "Epoch 61/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1414 - accuracy: 0.1633 - val_loss: 1.8397 - val_accuracy: 0.0796\n",
      "Epoch 62/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1389 - accuracy: 0.1638 - val_loss: 1.8692 - val_accuracy: 0.0800\n",
      "Epoch 63/100\n",
      "18755/18755 [==============================] - 35s 2ms/sample - loss: 0.1364 - accuracy: 0.1643 - val_loss: 1.8552 - val_accuracy: 0.0798\n",
      "Epoch 64/100\n",
      "18755/18755 [==============================] - 35s 2ms/sample - loss: 0.1343 - accuracy: 0.1645 - val_loss: 1.8568 - val_accuracy: 0.0798\n",
      "Epoch 65/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1318 - accuracy: 0.1650 - val_loss: 1.8693 - val_accuracy: 0.0796\n",
      "Epoch 66/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1296 - accuracy: 0.1655 - val_loss: 1.8598 - val_accuracy: 0.0801\n",
      "Epoch 67/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1272 - accuracy: 0.1658 - val_loss: 1.8711 - val_accuracy: 0.0799\n",
      "Epoch 68/100\n",
      "18755/18755 [==============================] - 33s 2ms/sample - loss: 0.1251 - accuracy: 0.1662 - val_loss: 1.8764 - val_accuracy: 0.0796\n",
      "Epoch 69/100\n",
      "18755/18755 [==============================] - 33s 2ms/sample - loss: 0.1234 - accuracy: 0.1664 - val_loss: 1.8737 - val_accuracy: 0.0797\n",
      "Epoch 70/100\n",
      "18755/18755 [==============================] - 33s 2ms/sample - loss: 0.1214 - accuracy: 0.1668 - val_loss: 1.8750 - val_accuracy: 0.0794\n",
      "Epoch 71/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1198 - accuracy: 0.1671 - val_loss: 1.9111 - val_accuracy: 0.0793\n",
      "Epoch 72/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1179 - accuracy: 0.1674 - val_loss: 1.8882 - val_accuracy: 0.0792\n",
      "Epoch 73/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1159 - accuracy: 0.1676 - val_loss: 1.9103 - val_accuracy: 0.0792\n",
      "Epoch 74/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1143 - accuracy: 0.1679 - val_loss: 1.8900 - val_accuracy: 0.0790\n",
      "Epoch 75/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1125 - accuracy: 0.1681 - val_loss: 1.8943 - val_accuracy: 0.0791\n",
      "Epoch 76/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1110 - accuracy: 0.1684 - val_loss: 1.8863 - val_accuracy: 0.0790\n",
      "Epoch 77/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1098 - accuracy: 0.1685 - val_loss: 1.9163 - val_accuracy: 0.0785\n",
      "Epoch 78/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1081 - accuracy: 0.1688 - val_loss: 1.9101 - val_accuracy: 0.0794\n",
      "Epoch 79/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1069 - accuracy: 0.1690 - val_loss: 1.9057 - val_accuracy: 0.0785\n",
      "Epoch 80/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1053 - accuracy: 0.1693 - val_loss: 1.9171 - val_accuracy: 0.0777\n",
      "Epoch 81/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1040 - accuracy: 0.1695 - val_loss: 1.8988 - val_accuracy: 0.0791\n",
      "Epoch 82/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1029 - accuracy: 0.1696 - val_loss: 1.9044 - val_accuracy: 0.0781\n",
      "Epoch 83/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1016 - accuracy: 0.1698 - val_loss: 1.9136 - val_accuracy: 0.0788\n",
      "Epoch 84/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.1003 - accuracy: 0.1700 - val_loss: 1.9208 - val_accuracy: 0.0783\n",
      "Epoch 85/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0992 - accuracy: 0.1702 - val_loss: 1.9250 - val_accuracy: 0.0784\n",
      "Epoch 86/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0985 - accuracy: 0.1703 - val_loss: 1.9174 - val_accuracy: 0.0787\n",
      "Epoch 87/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0972 - accuracy: 0.1704 - val_loss: 1.9035 - val_accuracy: 0.0786\n",
      "Epoch 88/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0959 - accuracy: 0.1708 - val_loss: 1.9049 - val_accuracy: 0.0792\n",
      "Epoch 89/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0948 - accuracy: 0.1707 - val_loss: 1.9226 - val_accuracy: 0.0780\n",
      "Epoch 90/100\n",
      "18755/18755 [==============================] - 33s 2ms/sample - loss: 0.0939 - accuracy: 0.1710 - val_loss: 1.9234 - val_accuracy: 0.0786\n",
      "Epoch 91/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0931 - accuracy: 0.1710 - val_loss: 1.9174 - val_accuracy: 0.0772\n",
      "Epoch 92/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0924 - accuracy: 0.1712 - val_loss: 1.9125 - val_accuracy: 0.0786\n",
      "Epoch 93/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0911 - accuracy: 0.1713 - val_loss: 1.9215 - val_accuracy: 0.0781\n",
      "Epoch 94/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0902 - accuracy: 0.1714 - val_loss: 1.9134 - val_accuracy: 0.0787\n",
      "Epoch 95/100\n",
      "18755/18755 [==============================] - 35s 2ms/sample - loss: 0.0893 - accuracy: 0.1716 - val_loss: 1.9274 - val_accuracy: 0.0785\n",
      "Epoch 96/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0888 - accuracy: 0.1715 - val_loss: 1.9123 - val_accuracy: 0.0785\n",
      "Epoch 97/100\n",
      "18755/18755 [==============================] - 35s 2ms/sample - loss: 0.0879 - accuracy: 0.1717 - val_loss: 1.9173 - val_accuracy: 0.0786\n",
      "Epoch 98/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0870 - accuracy: 0.1719 - val_loss: 1.9100 - val_accuracy: 0.0785\n",
      "Epoch 99/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0863 - accuracy: 0.1720 - val_loss: 1.9324 - val_accuracy: 0.0779\n",
      "Epoch 100/100\n",
      "18755/18755 [==============================] - 34s 2ms/sample - loss: 0.0849 - accuracy: 0.1722 - val_loss: 1.9275 - val_accuracy: 0.0783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23cfd9cd948>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, encoder_infer, decoder_infer = create_model(n_input=INPUT_FEATURE_LENGTH,\n",
    "                                                   n_output=OUTPUT_FEATURE_LENGTH,\n",
    "                                                   n_units=latent_dim)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input, decoder_input], decoder_output, \n",
    "          batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "model.save('seq2seq__chn_to_eng.h5')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 進行英翻中推論預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 42, 3420)\n",
      "我喜欢阅读\n",
      "i like reading \n",
      " \n",
      "(1, 42, 3420)\n",
      "我喜欢跑步\n",
      "i like running \n",
      " \n",
      "(1, 42, 3420)\n",
      "我喜欢阅读\n",
      "i like reading \n",
      " \n",
      "(1, 42, 3420)\n",
      "我爱我的生活\n",
      "i love my life \n",
      " \n",
      "(1, 42, 3420)\n",
      "我爱我的生命\n",
      "i love my life \n",
      " \n",
      "(1, 42, 3420)\n",
      "我爱我的妻子\n",
      "i love my wife \n",
      " \n",
      "(1, 42, 3420)\n",
      "我愛派對\n",
      "i love tom \n",
      " \n",
      "(1, 42, 3420)\n",
      "我遇见一个朋友\n",
      "i met a friend \n",
      " \n",
      "(1, 42, 3420)\n",
      "我必须拒绝\n",
      "i have to say no \n",
      " \n",
      "(1, 42, 3420)\n",
      "我该回家了\n",
      "i have to go home \n",
      " \n",
      "(1, 42, 3420)\n",
      "我需要加薪\n",
      "i need a stamp \n",
      " \n",
      "(1, 42, 3420)\n",
      "我需要一張郵票\n",
      "i need a stamp \n",
      " \n",
      "(1, 42, 3420)\n",
      "我尽快需要\n",
      "i need it as quickly as possible \n",
      " \n",
      "(1, 42, 3420)\n",
      "我需要我的大衣\n",
      "i need my coat \n",
      " \n",
      "(1, 42, 3420)\n",
      "我需要知道\n",
      "i need to know \n",
      " \n",
      "(1, 42, 3420)\n",
      "我经常打嗝\n",
      "i often visit \n",
      " \n",
      "(1, 42, 3420)\n",
      "我说过了闭嘴\n",
      "i said shut up \n",
      " \n",
      "(1, 42, 3420)\n",
      "我看到了五個男人\n",
      "i saw five men \n",
      " \n",
      "(1, 42, 3420)\n",
      "我看见五个男人\n",
      "i saw five men \n",
      " \n",
      "(1, 42, 3420)\n",
      "我看見她游泳\n",
      "i saw her swim \n",
      " \n",
      "(1, 42, 3420)\n",
      "我应该去做\n",
      "i should do it \n",
      " \n",
      "(1, 42, 3420)\n",
      "我学韩语\n",
      "i study play tennis \n",
      " \n",
      "(1, 42, 3420)\n",
      "我想去玩\n",
      "i want to play \n",
      " \n",
      "(1, 42, 3420)\n",
      "我以前是醫生\n",
      "i was a doctor \n",
      " \n",
      "(1, 42, 3420)\n",
      "我在學習\n",
      "i was learning \n",
      " \n",
      "(1, 42, 3420)\n",
      "我沒醉\n",
      "i was not drunk \n",
      " \n",
      "(1, 42, 3420)\n",
      "我要控告你\n",
      "i will show you what he is \n",
      " \n",
      "(1, 42, 3420)\n",
      "我想要去\n",
      "i would like to go \n",
      " \n",
      "(1, 42, 3420)\n",
      "我会警告汤姆\n",
      "i will tell tom \n",
      " \n",
      "(1, 42, 3420)\n",
      "我会回来的\n",
      "i will come back \n",
      " \n",
      "(1, 42, 3420)\n",
      "我會責備他\n",
      "i will buy him \n",
      " \n",
      "(1, 42, 3420)\n",
      "我會留意的\n",
      "i will see to it \n",
      " \n",
      "(1, 42, 3420)\n",
      "由我来做\n",
      "i will see to it \n",
      " \n",
      "(1, 42, 3420)\n",
      "我會待在家裡\n",
      "i will stay home \n",
      " \n",
      "(1, 42, 3420)\n",
      "我請你\n",
      "i will you in \n",
      " \n",
      "(1, 42, 3420)\n",
      "我是一個自由的人\n",
      "i am a free man \n",
      " \n",
      "(1, 42, 3420)\n",
      "我是一个好人\n",
      "i am a good guy \n",
      " \n",
      "(1, 42, 3420)\n",
      "我是无神论者\n",
      "i am an that \n",
      " \n",
      "(1, 42, 3420)\n",
      "我在他後面\n",
      "i am behind him \n",
      " \n",
      "(1, 42, 3420)\n",
      "我今天忙\n",
      "i am busy today \n",
      " \n",
      "(1, 42, 3420)\n",
      "我累死了\n",
      "i am dead tired \n",
      " \n",
      "(1, 42, 3420)\n",
      "我今天有空\n",
      "i am free today \n",
      " \n",
      "(1, 42, 3420)\n",
      "我不輕易放棄\n",
      "i am no wait \n",
      " \n",
      "(1, 42, 3420)\n",
      "我没名气\n",
      "i am not famous \n",
      " \n",
      "(1, 42, 3420)\n",
      "我不出名\n",
      "i am not famous \n",
      " \n",
      "(1, 42, 3420)\n",
      "我不貪婪\n",
      "i am not for \n",
      " \n",
      "(1, 42, 3420)\n",
      "我没有罪\n",
      "i am not as is as on \n",
      " \n",
      "(1, 42, 3420)\n",
      "我不是笨蛋\n",
      "i am not stupid \n",
      " \n",
      "(1, 42, 3420)\n",
      "我很激动\n",
      "i am so excited \n",
      " \n",
      "(1, 42, 3420)\n",
      "我脱衣服\n",
      "i am going to take a it \n",
      " \n",
      "(1, 42, 3420)\n",
      "我很快樂\n",
      "i am very happy \n",
      " \n",
      "(1, 42, 3420)\n",
      "我好高興\n",
      "i am so happy \n",
      " \n",
      "(1, 42, 3420)\n",
      "我很快乐\n",
      "i am very happy \n",
      " \n",
      "(1, 42, 3420)\n",
      "我很幸福\n",
      "i am very happy \n",
      " \n",
      "(1, 42, 3420)\n",
      "我見過\n",
      "i have seen that \n",
      " \n",
      "(1, 42, 3420)\n",
      "他是日本人吗\n",
      "is he japanese \n",
      " \n",
      "(1, 42, 3420)\n",
      "那更好吗\n",
      "is that better \n",
      " \n",
      "(1, 42, 3420)\n",
      "这个能吃吗\n",
      "can you eat this \n",
      " \n",
      "(1, 42, 3420)\n",
      "它不是黑色的嗎\n",
      "is not it black \n",
      " \n",
      "(1, 42, 3420)\n",
      "太早了\n",
      "it is so early \n",
      " \n",
      "(1, 42, 3420)\n",
      "太晚了\n",
      "it is too late \n",
      " \n",
      "(1, 42, 3420)\n",
      "那样说得通\n",
      "that makes sense \n",
      " \n",
      "(1, 42, 3420)\n",
      "它花了几个月\n",
      "it took months \n",
      " \n",
      "(1, 42, 3420)\n",
      "这是全新的\n",
      "it is new \n",
      " \n",
      "(1, 42, 3420)\n",
      "它是危險的\n",
      "it is dangerous \n",
      " \n",
      "(1, 42, 3420)\n",
      "很美味\n",
      "it is delicious \n",
      " \n",
      "(1, 42, 3420)\n",
      "这不好笑\n",
      "it is not funny \n",
      " \n",
      "(1, 42, 3420)\n",
      "它是我們的錯誤\n",
      "it is our fault \n",
      " \n",
      "(1, 42, 3420)\n",
      "这是事实\n",
      "it is the truth \n",
      " \n",
      "(1, 42, 3420)\n",
      "它太大了\n",
      "it is too large \n",
      " \n",
      "(1, 42, 3420)\n",
      "这就要看您了\n",
      "it is up to you \n",
      " \n",
      "(1, 42, 3420)\n",
      "由你來決定\n",
      "it is up to you to you \n",
      " \n",
      "(1, 42, 3420)\n",
      "非常冷\n",
      "it is very cold \n",
      " \n",
      "(1, 42, 3420)\n",
      "做得很好\n",
      "it is well done \n",
      " \n",
      "(1, 42, 3420)\n",
      "它是你的書\n",
      "it is your book \n",
      " \n",
      "(1, 42, 3420)\n",
      "該你走了\n",
      "it is your move \n",
      " \n",
      "(1, 42, 3420)\n",
      "輪到您了\n",
      "it is your turn \n",
      " \n",
      "(1, 42, 3420)\n",
      "輪到你了\n",
      "it is your turn \n",
      " \n",
      "(1, 42, 3420)\n",
      "继续听\n",
      "keep listening \n",
      " \n",
      "(1, 42, 3420)\n",
      "让我进去\n",
      "let me in \n",
      " \n",
      "(1, 42, 3420)\n",
      "让我去做\n",
      "let me do that \n",
      " \n",
      "(1, 42, 3420)\n",
      "让我们回家吧\n",
      "lets go home \n",
      " \n",
      "(1, 42, 3420)\n",
      "我們吃吧\n",
      "lets go \n",
      " \n",
      "(1, 42, 3420)\n",
      "聽聽這個\n",
      "listen to this \n",
      " \n",
      "(1, 42, 3420)\n",
      "请大声一点\n",
      "please take a little more \n",
      " \n",
      "(1, 42, 3420)\n",
      "午餐好了\n",
      "lunch is ready \n",
      " \n",
      "(1, 42, 3420)\n",
      "我能幫你嗎\n",
      "may i help you \n",
      " \n",
      "(1, 42, 3420)\n",
      "我可以用嗎\n",
      "may i use this \n",
      " \n",
      "(1, 42, 3420)\n",
      "我叫\n",
      "my name is tom \n",
      " \n",
      "(1, 42, 3420)\n",
      "现在我想起来了\n",
      "now i remember \n",
      " \n",
      "(1, 42, 3420)\n",
      "請進來\n",
      "please come in \n",
      " \n",
      "(1, 42, 3420)\n",
      "麻煩您那樣做\n",
      "please do that \n",
      " \n",
      "(1, 42, 3420)\n",
      "請幫助我\n",
      "please help me \n",
      " \n",
      "(1, 42, 3420)\n",
      "請幫我\n",
      "please help me \n",
      " \n",
      "(1, 42, 3420)\n",
      "請加入我們\n",
      "please join us \n",
      " \n",
      "(1, 42, 3420)\n",
      "請告訴我\n",
      "please tell me \n",
      " \n",
      "(1, 42, 3420)\n",
      "请清洗它\n",
      "please wash it \n",
      " \n",
      "(1, 42, 3420)\n",
      "物價上漲\n",
      "prices went up \n",
      " \n",
      "(1, 42, 3420)\n",
      "看这本书\n",
      "read this book \n",
      " \n",
      "(1, 42, 3420)\n",
      "說清楚\n",
      "say it clearly \n",
      " \n",
      "(1, 42, 3420)\n",
      "科學好玩\n",
      "be is fun \n",
      " \n",
      "(1, 42, 3420)\n",
      "再见\n",
      "see you \n",
      " \n",
      "(1, 42, 3420)\n",
      "她愚弄了他\n",
      "she not him \n",
      " \n",
      "(1, 42, 3420)\n",
      "她種了玫瑰\n",
      "she grew roses \n",
      " \n",
      "(1, 42, 3420)\n",
      "她抱了他\n",
      "she not him \n",
      " \n",
      "(1, 42, 3420)\n",
      "她在节食中\n",
      "she is on a diet \n",
      " \n",
      "(1, 42, 3420)\n",
      "她在节食\n",
      "she is on a diet \n",
      " \n",
      "(1, 42, 3420)\n",
      "她在行走\n",
      "she is walking \n",
      " \n",
      "(1, 42, 3420)\n",
      "她也許會來\n",
      "she might come \n",
      " \n",
      "(1, 42, 3420)\n",
      "她看來有錢\n",
      "she seems rich \n",
      " \n",
      "(1, 42, 3420)\n",
      "闭上你们的眼睛\n",
      "shut your eyes \n",
      " \n",
      "(1, 42, 3420)\n",
      "烟雾出现了\n",
      "smoke a \n",
      " \n",
      "(1, 42, 3420)\n",
      "有人打電話來\n",
      "someone called \n",
      " \n",
      "(1, 42, 3420)\n",
      "停止发牢骚吧\n",
      "stop \n",
      " \n",
      "(1, 42, 3420)\n",
      "停止抵抗\n",
      "stop \n",
      " \n",
      "(1, 42, 3420)\n",
      "夏天过去了\n",
      "summer is over \n",
      " \n",
      "(1, 42, 3420)\n",
      "你可以慢慢来\n",
      "take your time \n",
      " \n",
      "(1, 42, 3420)\n",
      "慢慢来\n",
      "take your time \n",
      " \n",
      "(1, 42, 3420)\n",
      "那样是错的\n",
      "that was wrong \n",
      " \n",
      "(1, 42, 3420)\n",
      "那是一個恥辱\n",
      "that is a shame \n",
      " \n",
      "(1, 42, 3420)\n",
      "那符合逻辑\n",
      "that is toms will \n",
      " \n",
      "(1, 42, 3420)\n",
      "那是我的大衣\n",
      "that is my coat \n",
      " \n",
      "(1, 42, 3420)\n",
      "那是完美的\n",
      "that is perfect \n",
      " \n",
      "(1, 42, 3420)\n",
      "太可惜了\n",
      "that is too bad \n",
      " \n",
      "(1, 42, 3420)\n",
      "多遗憾啊\n",
      "what a pity \n",
      " \n",
      "(1, 42, 3420)\n",
      "那太糟糕了\n",
      "that is too bad \n",
      " \n",
      "(1, 42, 3420)\n",
      "鳥兒歌唱\n",
      "the birds sang \n",
      " \n",
      "(1, 42, 3420)\n",
      "旗子升起了\n",
      "the am is long \n",
      " \n",
      "(1, 42, 3420)\n",
      "電話正在響\n",
      "the phone is ringing \n",
      " \n",
      "(1, 42, 3420)\n",
      "他們目光相接\n",
      "the their met fell \n",
      " \n",
      "(1, 42, 3420)\n",
      "這些是筆\n",
      "these are really good see you \n",
      " \n",
      "(1, 42, 3420)\n",
      "他們恨湯姆\n",
      "they want tom \n",
      " \n",
      "(1, 42, 3420)\n",
      "他們有工作\n",
      "they have been for \n",
      " \n",
      "(1, 42, 3420)\n",
      "他們讓我走\n",
      "they let me go \n",
      " \n",
      "(1, 42, 3420)\n",
      "他们喜欢那个\n",
      "they love that \n",
      " \n",
      "(1, 42, 3420)\n",
      "他们信任汤姆\n",
      "they trust tom \n",
      " \n",
      "(1, 42, 3420)\n",
      "他們想要更多\n",
      "they want more \n",
      " \n",
      "(1, 42, 3420)\n",
      "他們想要這個\n",
      "they want this \n",
      " \n",
      "(1, 42, 3420)\n",
      "他们不错\n",
      "they were good \n",
      " \n",
      "(1, 42, 3420)\n",
      "这是一本书\n",
      "this is a book \n",
      " \n",
      "(1, 42, 3420)\n",
      "那是我的包\n",
      "that is my money \n",
      " \n",
      "(1, 42, 3420)\n",
      "湯姆能改變\n",
      "tom can change \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆不会游泳\n",
      "tom cannot swim \n",
      " \n",
      "(1, 42, 3420)\n",
      "湯姆有個計畫\n",
      "tom has a plan \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆脑子好使\n",
      "tom has a good head on his shoulders \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆失败了\n",
      "tom has failed \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆是个拉比\n",
      "tom is a we \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆不傻\n",
      "tom is not stupid \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆醉了\n",
      "tom is drunk \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆醉了\n",
      "tom is drunk \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆不傻\n",
      "tom is not stupid \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆住在这里\n",
      "tom lives here \n",
      " \n",
      "(1, 42, 3420)\n",
      "湯姆看起來很蒼白\n",
      "tom looks pale \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆喜欢狗\n",
      "tom loves dogs \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆脸红了\n",
      "tom turned red \n",
      " \n",
      "(1, 42, 3420)\n",
      "湯姆走了出去\n",
      "tom walked out \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆当时在哭\n",
      "tom was crying \n",
      " \n",
      "(1, 42, 3420)\n",
      "湯姆不會停\n",
      "tom is not going to stop \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆无所畏惧\n",
      "toms got was \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆在笑\n",
      "tom is laughing \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆正昏迷不醒\n",
      "tom is out cold \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆兴奋不已\n",
      "tom is no \n",
      " \n",
      "(1, 42, 3420)\n",
      "开电视\n",
      "turn on the tv \n",
      " \n",
      "(1, 42, 3420)\n",
      "把电视声音调大点儿\n",
      "turn up the tv \n",
      " \n",
      "(1, 42, 3420)\n",
      "汤姆睡着了吗\n",
      "was tom asleep \n",
      " \n",
      "(1, 42, 3420)\n",
      "洗您的脚\n",
      "wash your feet \n",
      " \n",
      "(1, 42, 3420)\n",
      "洗你的脚\n",
      "wash your feet \n",
      " \n",
      "(1, 42, 3420)\n",
      "自己当心啊\n",
      "watch yourself \n",
      " \n",
      "(1, 42, 3420)\n",
      "我們原諒你\n",
      "we forgive you \n",
      " \n",
      "(1, 42, 3420)\n",
      "我們誰也不認識\n",
      "we knew no one \n",
      " \n",
      "(1, 42, 3420)\n",
      "我們需要英雄\n",
      "we need a a do \n",
      " \n",
      "(1, 42, 3420)\n",
      "我们学习音乐\n",
      "we study music \n",
      " \n",
      "(1, 42, 3420)\n",
      "我們被偷了\n",
      "we were caught \n",
      " \n",
      "(1, 42, 3420)\n",
      "我们要继续下去\n",
      "we will continue \n",
      " \n",
      "(1, 42, 3420)\n",
      "我們是個家庭\n",
      "we are a family \n",
      " \n",
      "(1, 42, 3420)\n",
      "我们没迟到\n",
      "we are not late \n",
      " \n",
      "(1, 42, 3420)\n",
      "好吧我們走吧\n",
      "well lets go \n",
      " \n",
      "(1, 42, 3420)\n",
      "你是对的吗\n",
      "are you right \n",
      " \n",
      "(1, 42, 3420)\n",
      "你們呢\n",
      "how about you \n",
      " \n",
      "(1, 42, 3420)\n",
      "您呢\n",
      "how about you \n",
      " \n",
      "(1, 42, 3420)\n",
      "你的職業是什麼\n",
      "that is your business \n",
      " \n",
      "(1, 42, 3420)\n",
      "她做什么工作\n",
      "that is her job \n",
      " \n",
      "(1, 42, 3420)\n",
      "我们去哪儿\n",
      "where do we go \n",
      " \n",
      "(1, 42, 3420)\n",
      "之前你在哪里\n",
      "where were you \n",
      " \n",
      "(1, 42, 3420)\n",
      "那家伙是谁\n",
      "whos that guy \n",
      " \n",
      "(1, 42, 3420)\n",
      "那男人是谁\n",
      "whos that man \n",
      " \n",
      "(1, 42, 3420)\n",
      "你問這個幹什麼\n",
      "why do you ask \n",
      " \n",
      "(1, 42, 3420)\n",
      "为什么他在这儿\n",
      "why is he here \n",
      " \n",
      "(1, 42, 3420)\n",
      "擦擦你的眼睛\n",
      "cannot your eyes \n",
      " \n",
      "(1, 42, 3420)\n",
      "是的我知道\n",
      "that is not know \n",
      " \n",
      "(1, 42, 3420)\n",
      "是的當然\n",
      "yes of course \n",
      " \n",
      "(1, 42, 3420)\n",
      "你看起來很無聊\n",
      "you look bored \n",
      " \n",
      "(1, 42, 3420)\n",
      "你看起来很紧张\n",
      "you look very nervous \n",
      " \n",
      "(1, 42, 3420)\n",
      "你看起來很疲倦\n",
      "you look tired \n",
      " \n",
      "(1, 42, 3420)\n",
      "你看起来很困了\n",
      "you look tired \n",
      " \n",
      "(1, 42, 3420)\n",
      "你必須去做\n",
      "you must do it \n",
      " \n",
      "(1, 42, 3420)\n",
      "你會愛它\n",
      "you will love it \n",
      " \n",
      "(1, 42, 3420)\n",
      "你開玩笑吧\n",
      "you are joking \n",
      " \n",
      "(1, 42, 3420)\n",
      "您不必感谢我\n",
      "you do not need to thank me \n",
      " \n",
      "(1, 42, 3420)\n",
      "男人应该工作\n",
      "the man must work \n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(1000, 1200):\n",
    "    test = encoder_input[i:i+1, :, :]\n",
    "    print(test.shape)\n",
    "    out = predict_chinese(test, encoder_infer, decoder_infer, OUTPUT_LENGTH, OUTPUT_FEATURE_LENGTH)\n",
    "    print(input_sentences[i])\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
